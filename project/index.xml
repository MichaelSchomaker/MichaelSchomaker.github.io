<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Michael Schomaker</title>
    <link>https://MichaelSchomaker.github.io/project/</link>
      <atom:link href="https://MichaelSchomaker.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 04 Jan 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://MichaelSchomaker.github.io/images/icon_hue888863bcf6448bbe1c2d7b1880687e8_12435_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://MichaelSchomaker.github.io/project/</link>
    </image>
    
    <item>
      <title>MAMI</title>
      <link>https://MichaelSchomaker.github.io/project/mami/</link>
      <pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://MichaelSchomaker.github.io/project/mami/</guid>
      <description>&lt;p&gt;&lt;strong&gt;MAMI - An  
  &lt;i class=&#34;fab fa-r-project  pr-1 fa-fw&#34;&gt;&lt;/i&gt; package for Model Averaging (and Model Selection) after Multiple Imputation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#Background&#34;&gt;$\rightarrow$ Background&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#Software&#34;&gt;$\rightarrow$ Software&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#other&#34;&gt;$\rightarrow$ Further considerations and literature&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;Background&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;
&lt;h4 id=&#34;model-averaging&#34;&gt;Model Averaging&lt;/h4&gt;
&lt;p&gt;The motivation for variable selection in regression models is based on the rationale that associational relationships between variables are best understood by reducing the model&amp;rsquo;s dimension. 
The problem with this approach is that (i) regression parameters after model selection are often biased and (ii) the respective standard errors are too small because they do not reflect the uncertainty 
related to the model selection process. It has been proposed  that the drawback of model selection can be overcome by model averaging. With model averaging, one calculates a weighted average 
$\hat{\bar{\beta}} = \sum_{\kappa=1}^{k} w_{\kappa} \hat{\beta}_{\kappa}$ from $k$ parameter estimates of a set of candidate (regression) models $\mathcal{M}={M_1,\ldots,M_k}$,
where the weights are calculated in a way such that better models receive a higher weight.&lt;/p&gt;
&lt;p&gt;A popular weight choice would be based on the exponential AIC,&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
w_{\kappa}^{\text{AIC}} &amp;amp;=&amp;amp; \frac{\exp(-\frac{1}{2} \mathrm{AIC_{\kappa})}}{\sum_{\kappa=1}^k\exp(-\frac{1}{2} \mathrm{AIC_{\kappa})}} ,
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;where $\mathrm{AIC_{\kappa}}$ is the AIC value related to model $M_{\kappa}\in\mathcal{M}$ and $\sum_{\kappa} w_{\kappa}^{\text{AIC}} =1$. It has been suggested
to estimate the variance of the scalar $\hat{\bar{\beta}}_j \in \hat{\bar{{\beta}}}$ via&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
\widehat{\text{Var}}(\hat{\bar{\beta}}&lt;em&gt;j) &amp;amp;=&amp;amp; {\sum&lt;/em&gt;{\kappa=1}^k w_{\kappa} \sqrt{\widehat{\text{Var}}(\hat{\beta}&lt;em&gt;{j,\kappa}|M&lt;/em&gt;{\kappa}) + (\hat{\beta}_{j,\kappa}-\hat{\bar{\beta}}_j})^2}^2 ,
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;where $\hat{\beta}_{j,\kappa}$ is the j$^{th}$ regression coefficient of the $\kappa^{th}$ candidate model. This approach tackles problem (ii), the incorporation of model selection uncertainty into the standard errors of the regression parameters; but it may not necessarily tackle problem (i) as the regression parameters may still be biased.
There are multiple different suggestions on how the weights can be calculated, and those implemented in &amp;ldquo;MAMI&amp;rdquo; are explained in the &lt;a href=&#34;https://MichaelSchomaker.github.io/pdf/MAMI_manual.pdf&#34; target=&#34;_blank&#34;&gt;manual&lt;/a&gt;.
Note that model selection can be viewed as a special case of model averaging where the ``best&#39;&#39; model receives weight $1$ (and all others a weight of $0$). All implemented model selection options are listed in the  &lt;a href=&#34;https://MichaelSchomaker.github.io/pdf/MAMI_manual.pdf&#34; target=&#34;_blank&#34;&gt;manual&lt;/a&gt; too.&lt;/p&gt;
&lt;h4 id=&#34;multiple-imputation&#34;&gt;Multiple Imputation&lt;/h4&gt;
&lt;p&gt;Multiple imputation (MI) is a popular method to address missing data. Based on assumptions about the data distribution (and the mechanism which gives rise to the missing data) missing values can be 
imputed by means of draws from the posterior predictive distribution of the unobserved data given the observed data. This procedure is repeated to create $M$ imputed data sets, the (regression) 
analysis is then conducted on each of these data sets and the $M$ results ($M$ point and $M$ variance estimates) are combined by a set of simple rules:&lt;/p&gt;
&lt;p&gt;\begin{eqnarray}
\hat{\beta}_{\text{MI}} &amp;amp;=&amp;amp;  \frac{1}{M} \sum_{m=1}^M \hat{\beta}^{(m)}
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;and
\begin{eqnarray}
\widehat{{\text{Cov}}}(\hat{\beta}_{\text{MI}}) &amp;amp;=&amp;amp; \frac{1}{M} \sum_{m=1}^{M} \widehat{\text{Cov}}(\hat{\beta}^{(m)}) + \frac{M+1}{M(M-1)} \sum_{m=1}^{M} (\hat{\beta}^{(m)}-\hat{\beta}_{\text{MI}}) (\hat{\beta}^{(m)}-\hat{\beta}_{\text{MI}})^{&#39;}
\end{eqnarray}&lt;/p&gt;
&lt;p&gt;where $ \hat{\beta}^{(m)}$ refers to the estimate of $\beta$ in the $\text{m}^{th}$ imputed set of data. Confidence intervals are based on a particular $t$-distribution.&lt;/p&gt;
&lt;h4 id=&#34;model-averaging-or-model-selection-after-multiple-imputation&#34;&gt;Model Averaging (or Model Selection) after Multiple Imputation&lt;/h4&gt;
&lt;p&gt;How can model averaging and model selection be applied to multiply imputed data? The detailed motivation can be found in the 2014 reference 
&lt;a href=&#34;#bottom&#34;&gt;below&lt;/a&gt;. The basic results for model &lt;em&gt;averaging&lt;/em&gt; are
\begin{eqnarray}
\hat{\bar{\beta}}_{\text{MI}} &amp;amp;=&amp;amp; \frac{1}{M} \sum_{m=1}^{M} \hat{\bar{\beta}}^{(m)}\quad \text{with} \quad \hat{\bar{{\beta}}}^{(m)} = \sum_{\kappa=1}^{k} w_{\kappa}^{(m)} \hat{{\beta}}_{\kappa}^{(m)}
\end{eqnarray}
and applies to any weight choice. If the variance of the model averaging estimator is estimated via the formula given above, the overall variance of the estimator after multiple imputation relates to
\begin{eqnarray}
\widehat{\text{Var}}(\hat{\bar{\beta}}_{j,\text{MI}}) &amp;amp;=&amp;amp; \frac{1}{M} \sum_{m=1}^{M}  {\sum_{\kappa=1}^k w_{\kappa} \sqrt{\widehat{\text{Var}}(\hat{\beta}_{j,\kappa}^{(m)})+(\hat{\beta}_{j,\kappa}^{(m)}-\hat{\bar{\beta}}_j^{(m)})^2} }^2  + \newline 
&amp;amp;&amp;amp;\frac{M+1}{M(M-1)} \sum_{m=1}^{M} (\hat{\bar\beta}_j^{(m)}-\hat{\bar\beta}_{j,\text{MI}})^2 .
\end{eqnarray}
Confidence intervals could then again be estimated based on a $t$ distribution (as explained above) or, alternatively, via bootsrapping (see 2014 reference 
&lt;a href=&#34;#bottom&#34;&gt;below&lt;/a&gt;, and 
&lt;a href=&#34;https://MichaelSchomaker.github.io/publication/2018-01-01_bootstrap_inference_/&#34;&gt;this&lt;/a&gt; publication of mine for a general Bootstrap-MI framework).&lt;/p&gt;
&lt;p&gt;Model &lt;em&gt;selection&lt;/em&gt; after imputation works essentially the same, except that parameters associated with variables which have not been selected are assumed to be $0$. With this assumption, a variable will be formally selected 
if it is selected in at least one imputed set of data, but its overall impact will depend on how often it is chosen. Here, confidence intervals will almost always be too narrow if 
the formula from above will be applied (because of model selection uncertainty) and bootstrapping is strongly recommended.&lt;/p&gt;
&lt;p&gt;As a consequence for model selection (and model averaging), effects of variables which are not supported throughout imputed data sets (and candidate models) will simply be less pronounced.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;Software&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;software&#34;&gt;Software&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;MAMI&lt;/em&gt; estimates the point estimates as explained above, together with confidence intervals that are either based on the formula above, or a Bayesian variation thereof, or bootstrapping (preferred option).&lt;/p&gt;
&lt;p&gt;In addition, a variable importance measure (averaged over the $M$ imputed data sets) will be calculated: this measure simply sums up the weights $w_{\kappa}$ of those candidate models $M_{\kappa}$ that contain the relevant variable,
and lies between $0$ (unimportant) and $1$ (very important). It is similar to the Bayesian posterior effect probability. Results can be interpreted and reported as suggested in the &lt;a href=&#34;https://MichaelSchomaker.github.io/pdf/MAMI_manual.pdf&#34; target=&#34;_blank&#34;&gt;manual&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;MAMI&lt;/em&gt;&amp;rsquo;s main function is &lt;em&gt;mami()&lt;/em&gt;. It is recommended to get familiar with the function&amp;rsquo;s syntax by typing &lt;em&gt;?mami&lt;/em&gt;, running the examples at the bottom of the help page and read through the manual.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The package can be downloaded 
&lt;a href=&#34;http://mami.r-forge.r-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Install it via&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;install.packages(&amp;quot;MAMI&amp;quot;, 
	repos=c(&amp;quot;http://R-Forge.R-project.org&amp;quot;,&amp;quot;http://cran.at.r-project.org&amp;quot;),
	dependencies=TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The manual can be found &lt;a href=&#34;https://MichaelSchomaker.github.io/pdf/MAMI_manual.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The package provides access to less frequently used model averaging techniques, offers integrated bootstrap estimation and easy-to-use parallelization.&lt;/li&gt;
&lt;li&gt;Optimal model averaging procedures are implemented too, and wrappers for their use in &lt;em&gt;super learning&lt;/em&gt; are integrated into the package. See also 
&lt;a href=&#34;https://MichaelSchomaker.github.io/project/sae/&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a name=&#34;other&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;further-considerations&#34;&gt;Further considerations&lt;/h3&gt;
&lt;p&gt;Interpretation of regression parameters requires care: for any explanatory interpretation causal considerations need to be taken into account (see my paper on regression and causailty 

&lt;a href=&#34;https://MichaelSchomaker.github.io/publication/2020-06-25_regression_causality/&#34;&gt;here&lt;/a&gt; ). However, both model averaging and post model selection estimators are biased; thus, in most cases a causal interpretations are invalid. 
Even when confidence interval coverage is improved, as explained above, it does not change the fact that in many cases model averaging and model selection estimators are more suitable for predictive tasks. 
Modern doubly robust etimators, such as targeted maximum likelihood estimators, can integrate data-adaptive approaches which are based on predictive considerations, while still retaining valid inference. Therefore,
the integration of model averaging estimators in machine learning approaches which are used for causal effect estimation can be attractive as explained in 
&lt;a href=&#34;https://MichaelSchomaker.github.io/publication/2019-01-01_when_and_when_not_to/&#34;&gt;this&lt;/a&gt;
recent paper of mine. &lt;em&gt;MAMI&lt;/em&gt; offers a couple of wrappers that can be used to integrate different model averaging approaches into super learning, a data-adaptive approach which uses a weighted combination of different learning algorithms
and is popular in causal inference. See Section 6.3. of the &lt;a href=&#34;https://MichaelSchomaker.github.io/pdf/MAMI_manual.pdf&#34; target=&#34;_blank&#34;&gt;manual&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;bottom&#34;&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CICI</title>
      <link>https://MichaelSchomaker.github.io/project/cici/</link>
      <pubDate>Wed, 04 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://MichaelSchomaker.github.io/project/cici/</guid>
      <description>&lt;p&gt;&lt;strong&gt;CICI&lt;/strong&gt; is an 
  &lt;i class=&#34;fab fa-r-project  pr-1 fa-fw&#34;&gt;&lt;/i&gt; package for &lt;strong&gt;C&lt;/strong&gt;ausal &lt;strong&gt;I&lt;/strong&gt;nference with &lt;strong&gt;C&lt;/strong&gt;ontinuous &lt;strong&gt;I&lt;/strong&gt;nterventions.&lt;/p&gt;
&lt;p&gt;It facilitates estimation of counterfactual outcomes for multiple values of continuous interventions at different time points, and allows plotting of causal dose-response curves (CDRC). It can address positivity violations through weight functions.&lt;/p&gt;
&lt;p&gt;The package implements the methods described in Schomaker, McIlleron, Denti and Díaz (2023), see  
&lt;a href=&#34;https://MichaelSchomaker.github.io/publication/2023-05-12_continuous_intervention/&#34;&gt;here&lt;/a&gt;. It can be used for non-continuous interventions too. Briefly, the paper describes two approaches: the first one is to use 
standard parametric $g$-computation, where one intervenes on multiple values of the continuous intervention, at each time point. This approach may be complemented with diagnostics to understand the severity of positivity violations. An alternative approach 
is to use sequential $g$-computation where outcome weights redefine the estimand in regions of low support. Note that developing $g-$formula-based approaches for continuous longitudinal interventions is an obvious suggestion because developing a standard doubly robust estimator, e.g.
a TMLE, is not possible as the CDRC is not a pathwise-differentiable parameter; and developing non-standard doubly robust estimators is not straightforward in a multiple time-point setting.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#gformula&#34;&gt;$\rightarrow$ standard parametric $g$-formula&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#sgf&#34;&gt;$\rightarrow$ weighted sequential $g$-formula&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#code&#34;&gt;$\rightarrow$ example code&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;
&lt;p&gt;The package is available on CRAN. The weighted sequential approach is currently only available on GitHub and needs to be installed seperately, but will be integrated into the main package later:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;install.packages(&amp;quot;CICI&amp;quot;)
library(remotes)
remotes::install_github(&amp;quot;MichaelSchomaker/CICIplus&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a name=&#34;gformula&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;standard-g-computation-for-continuous-interventions&#34;&gt;Standard $g$-computation for continuous interventions&lt;/h3&gt;
&lt;h4 id=&#34;standard-application&#34;&gt;Standard application&lt;/h4&gt;
&lt;p&gt;As an example, consider the efavirenz data contained in the package. This simulated data set is similar to the real data set described in Schomaker et al. (2023; see paper on more details, including identification considerations). We are interested in what the probability of viral failure (binary outcome) at time $t$ would be
if the concentration was $a$ mg/l for everyone ($a=0,&amp;hellip;,10$), at each time point. If the intervention values do not change over time, we can simply specify a vector under $\texttt{abar}$ that contains the values with which we 
want to intervene. Apart from this, only the dataset ($\texttt{X}$), time-varying confounders ($\texttt{Lnodes}$), outcome variables ($\texttt{Ynodes}$) and intervention variables ($\texttt{Anodes}$) need to be specifed. Baseline variables are detected
automatically:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;library(CICI)
data(EFV)

# parametric g-formula
est &amp;lt;- gformula(X=EFV,
                Lnodes  = c(&amp;quot;adherence.1&amp;quot;,&amp;quot;weight.1&amp;quot;,
                            &amp;quot;adherence.2&amp;quot;,&amp;quot;weight.2&amp;quot;,
                            &amp;quot;adherence.3&amp;quot;,&amp;quot;weight.3&amp;quot;,
                            &amp;quot;adherence.4&amp;quot;,&amp;quot;weight.4&amp;quot;),
                Ynodes  = c(&amp;quot;VL.0&amp;quot;,&amp;quot;VL.1&amp;quot;,&amp;quot;VL.2&amp;quot;,&amp;quot;VL.3&amp;quot;,&amp;quot;VL.4&amp;quot;),
                Anodes  = c(&amp;quot;efv.0&amp;quot;,&amp;quot;efv.1&amp;quot;,&amp;quot;efv.2&amp;quot;,&amp;quot;efv.3&amp;quot;,&amp;quot;efv.4&amp;quot;),
                abar=seq(0,10,1)
)

est        # print estimates
plot(est)  # plot estimates
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The $\texttt{plot}$ functionality can be used to plot the estimated counterfactual outcomes in terms of dose-response curves at different time points. In the below example, the curve at $t=5$ means that we look at $Y_5 = \text{VL}.4$ under the interventions 
$(0,0,0,0,0),&amp;hellip;,(10,10,10,10,10)$:&lt;/p&gt;
&lt;div&gt;&lt;img src=&#34;https://MichaelSchomaker.github.io/img/CDRC_1.jpeg&#34;, width=100%&gt;&lt;/div&gt;
&lt;p&gt;More complex custom interventions that change over time can be specified too under $\texttt{abar}$, and they will be plotted differently; type $\texttt{?gformula}$ for details.&lt;/p&gt;
&lt;h4 id=&#34;bootstrap-confidence-intervals&#34;&gt;Bootstrap confidence intervals&lt;/h4&gt;
&lt;p&gt;To obtain bootstrap confidence intervals, simply specify the number of bootstrap samples in the option $\texttt{B}$. CI&amp;rsquo;s can also be plotted. Examples are given in the example code listed

&lt;a href=&#34;#code&#34;&gt;further below&lt;/a&gt; (parallelization using the option $\texttt{ncores}$ is recommended).&lt;/p&gt;
&lt;h4 id=&#34;other-estimands&#34;&gt;Other estimands&lt;/h4&gt;
&lt;p&gt;In $\texttt{CICI}$, it is easy to estimate and plot other estimands than $E(Y^{abar})$. Simply, i) set the option $\texttt{ret=T}$ in $\texttt{gformula()}$, which saves the counterfactual datasets under the different interventions; and then, ii) apply the integrated $\texttt{custom.measure}$ function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;custom.measure(est, fun=prop, categ=1) # P(Y^a=1) - &amp;gt; identical results
custom.measure(est, fun=prop, categ=0) # P(Y^a=0)
custom.measure(est, fun=prop, categ=0, cond=&amp;quot;sex==1&amp;quot;) # conditional on sex=1

# does not make sense here, just for illustration:
custom.measure(est, fun=quantile, probs=0.1) # counterfactual quantiles
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;model-specification&#34;&gt;Model specification&lt;/h4&gt;
&lt;p&gt;The longitudinal parametric $g$-formula requires fitting of both confounder and outcome models at each time point. By default (as a starting point) GLM&amp;rsquo;s with all variables included are used, though CICI is implemented in a more general and flexible GAM framework. The model
families are picked by $\texttt{CICI}$ automatically and are restricted to those available within the $\texttt{mgcv}$ package to fit generalized additive models. As correct model specification is imperative for the success of $g$-formula type of approaches, variable screening and the inclusion of non-linear and interaction terms is often important. The package comes with built-in wrappers to 
help with this. A recommended approach is to start as follows: 1) generate (strings of) all relevant model formulae with the function $\texttt{make.model.formulas}$, then 2) screen variables with LASSO using $\texttt{model.formulas.update}$ and
then 3) pass on the updated formulae (possibly aftr furthr manual updates, see below) to $\texttt{gformula}$ using the $\texttt{Yform, Lform, Aform, Cform}$ options, as required by the estimand.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;# 1) Generic model formulas
m &amp;lt;- make.model.formulas(X=EFV,
                         Lnodes  = c(&amp;quot;adherence.1&amp;quot;,&amp;quot;weight.1&amp;quot;,
                                     &amp;quot;adherence.2&amp;quot;,&amp;quot;weight.2&amp;quot;,
                                     &amp;quot;adherence.3&amp;quot;,&amp;quot;weight.3&amp;quot;,
                                     &amp;quot;adherence.4&amp;quot;,&amp;quot;weight.4&amp;quot;),
                         Ynodes  = c(&amp;quot;VL.0&amp;quot;,&amp;quot;VL.1&amp;quot;,&amp;quot;VL.2&amp;quot;,&amp;quot;VL.3&amp;quot;,&amp;quot;VL.4&amp;quot;),
                         Anodes  = c(&amp;quot;efv.0&amp;quot;,&amp;quot;efv.1&amp;quot;,&amp;quot;efv.2&amp;quot;,&amp;quot;efv.3&amp;quot;,&amp;quot;efv.4&amp;quot;)
)
m$model.names # all models potentially relevant for gformula(), given *full* past

# 2) Update model formulas (automated): screening with LASSO 
glmnet.formulas &amp;lt;-  model.formulas.update(m$model.names, EFV)

# 3) use these models in gformula()
est &amp;lt;- gformula(...,
                Yform=glmnet.formulas$Ynames, Lform=glmnet.formulas$Lnames,
                ...)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, even with this approach one may have to still manually improve the models, for example by adding (penalized) splines and interactions. Examples are given at the bottom of the following help pages:
$\texttt{?fit.updated.formulas, ?model.update}$. A future manual will give more details on possibilities of model specification.&lt;/p&gt;
&lt;h4 id=&#34;multiple-imputation&#34;&gt;Multiple imputation&lt;/h4&gt;
&lt;p&gt;CICI can deal with multiply imputed data. Currently the option MI Boot, explained in the publication 
&lt;a href=&#34;#bottom&#34;&gt;further below&lt;/a&gt; (Schomaker and Heumann, 2018), is implemented. Simply use $\texttt{gformula}$ on each imputed data set and then use the 
$\texttt{mi.boot}$ command (custom estimands are possible too after imputation):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;# suppose the following subsets were actually multiply imputed data (M=2)
EFV_1 &amp;lt;- EFV[1:2500,]
EFV_2 &amp;lt;- EFV[2501:5000,]

# first: conduct analysis on each imputed data set. Set ret=T.
m1 &amp;lt;- gformula(..., ret=T)
m2 &amp;lt;- gformula(..., ret=T)

# second: combine results
m_imp &amp;lt;- mi.boot(list(m1,m2), mean) # uses MI rules &amp;amp; returns &#39;gformula&#39; object
plot(m_imp)

# custom estimand: evaluate probability of suppression (Y=0), among females
m_imp2 &amp;lt;- mi.boot(list(m1,m2), prop, categ=0, cond=&amp;quot;sex==1&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;other-useful-options&#34;&gt;Other useful options&lt;/h4&gt;
&lt;p&gt;There are many more useful option, wich are explained in $\texttt{?gformula}$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to handle &lt;strong&gt;survival&lt;/strong&gt; settings&lt;/li&gt;
&lt;li&gt;How to generate diagnostics&lt;/li&gt;
&lt;li&gt;How to specify both custom and natural interventions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallelization&lt;/strong&gt; using the option $\texttt{ncores}$ (very easy!)&lt;/li&gt;
&lt;li&gt;How to track &lt;strong&gt;progress&lt;/strong&gt;, even under parallelizations with $\texttt{prog}$&lt;/li&gt;
&lt;li&gt;How to reproduce results using $\texttt{seed}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The package can of course also handle binary and categorical interventions. Dynamic interventions are not supported.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;sgf&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;weighted-sequential-g-computation&#34;&gt;Weighted sequential $g$-computation&lt;/h3&gt;
&lt;h4 id=&#34;basic-unweighted-setup&#34;&gt;Basic (unweighted) setup&lt;/h4&gt;
&lt;p&gt;The function $\texttt{sgf()}$ implements the sequential $g$-formula and works very similar as $\texttt{gformula()}$. The below example illustrates this. The estimated CDRC&amp;rsquo;s are very similar.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;library(CICIplus) 

# (unweighted) sequential g-formula
est2 &amp;lt;- sgf(X=EFV,
            Lnodes  = c(&amp;quot;adherence.1&amp;quot;,&amp;quot;weight.1&amp;quot;,
                        &amp;quot;adherence.2&amp;quot;,&amp;quot;weight.2&amp;quot;,
                        &amp;quot;adherence.3&amp;quot;,&amp;quot;weight.3&amp;quot;,
                        &amp;quot;adherence.4&amp;quot;,&amp;quot;weight.4&amp;quot;),
            Ynodes  = c(&amp;quot;VL.0&amp;quot;,&amp;quot;VL.1&amp;quot;,&amp;quot;VL.2&amp;quot;,&amp;quot;VL.3&amp;quot;,&amp;quot;VL.4&amp;quot;),
            Anodes  = c(&amp;quot;efv.0&amp;quot;,&amp;quot;efv.1&amp;quot;,&amp;quot;efv.2&amp;quot;,&amp;quot;efv.3&amp;quot;,&amp;quot;efv.4&amp;quot;),
            abar=seq(0,10,1)
)
est2
plot(est2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;calculating-outcome-weights&#34;&gt;Calculating outcome weights&lt;/h4&gt;
&lt;p&gt;The function $\texttt{calc.weights}$ calculates the outcome weights described in equation (12) of Schomaker et al. (2023). The setup is simple: i) define variables as confounders, outcomes, interventions or censoring indicators; 
ii) choose a method for conditional density estimation, iii) specify one or many values for $c$, to specify how low support regions are defined:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;# Step 1: calculate weights (add baseline variables to Lnodes)
# a) parametric density based
w &amp;lt;- calc.weights(X=EFV, 
                         Lnodes  = c(&amp;quot;sex&amp;quot;, &amp;quot;metabolic&amp;quot;, &amp;quot;log_age&amp;quot;,
                                     &amp;quot;NRTI&amp;quot; ,&amp;quot;weight.0&amp;quot;,
                                     &amp;quot;adherence.1&amp;quot;,&amp;quot;weight.1&amp;quot;,
                                     &amp;quot;adherence.2&amp;quot;,&amp;quot;weight.2&amp;quot;,
                                     &amp;quot;adherence.3&amp;quot;,&amp;quot;weight.3&amp;quot;,
                                     &amp;quot;adherence.4&amp;quot;,&amp;quot;weight.4&amp;quot;),
                         Ynodes  = c(&amp;quot;VL.0&amp;quot;,&amp;quot;VL.1&amp;quot;,&amp;quot;VL.2&amp;quot;,&amp;quot;VL.3&amp;quot;,&amp;quot;VL.4&amp;quot;),
                         Anodes  = c(&amp;quot;efv.0&amp;quot;,&amp;quot;efv.1&amp;quot;,&amp;quot;efv.2&amp;quot;,&amp;quot;efv.3&amp;quot;,&amp;quot;efv.4&amp;quot;),
                         d.method=&amp;quot;binning&amp;quot;, abar=seq(0,10,1),
                         c=c(0.01,0.001)
)
summary(w) # weight summary
# d.method alternatives: parametric density estimation or highly-adaptive LASSO based
# check ?calc.weights for details
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;weighted-sequential-g-formula&#34;&gt;Weighted sequential $g$-formula&lt;/h4&gt;
&lt;p&gt;We can now use the calculated weights in the sequential $g$-formula by using the $\texttt{Yweights}$ option. The estimation algorithm is listed in Table 1 of Schomaker et al. (2023):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;# Step 2: sequential g-formula with outcome weights
est3 &amp;lt;- sgf(X=EFV,
              Lnodes  = c(&amp;quot;adherence.1&amp;quot;,&amp;quot;weight.1&amp;quot;,
                          &amp;quot;adherence.2&amp;quot;,&amp;quot;weight.2&amp;quot;,
                          &amp;quot;adherence.3&amp;quot;,&amp;quot;weight.3&amp;quot;,
                          &amp;quot;adherence.4&amp;quot;,&amp;quot;weight.4&amp;quot;),
              Ynodes  = c(&amp;quot;VL.0&amp;quot;,&amp;quot;VL.1&amp;quot;,&amp;quot;VL.2&amp;quot;,&amp;quot;VL.3&amp;quot;,&amp;quot;VL.4&amp;quot;),
              Anodes  = c(&amp;quot;efv.0&amp;quot;,&amp;quot;efv.1&amp;quot;,&amp;quot;efv.2&amp;quot;,&amp;quot;efv.3&amp;quot;,&amp;quot;efv.4&amp;quot;),
              Yweights = w$`0.01`,
              abar=seq(0,10,1)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;super-learning&#34;&gt;Super Learning&lt;/h4&gt;
&lt;p&gt;The sequential $g$-formula needs models for the iterated nested conditional outcomes. In the case of binary outcome variables (and more than 1 time point) this equates to modeling proportional data as of the second iteration step.
Using a data-adaptive approach, with super learning, is thus a good option. Candidate learners, and screening algorithms, can be easily passed on to the $\texttt{SuperLearner}$ package. Note that user-written learners (as in the example below)
need to be listed in the option $\texttt{SL.export}$ if parallelization is used (thanks to Eric Polley for helping me here!).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;# Realistic setup
# (requires user-written learners which are available upon request)

library(SuperLearner) 
est5 &amp;lt;- sgf(X=EFV,
            Lnodes  = c(&amp;quot;adherence.1&amp;quot;,&amp;quot;weight.1&amp;quot;,
                        &amp;quot;adherence.2&amp;quot;,&amp;quot;weight.2&amp;quot;,
                        &amp;quot;adherence.3&amp;quot;,&amp;quot;weight.3&amp;quot;,
                        &amp;quot;adherence.4&amp;quot;,&amp;quot;weight.4&amp;quot;),
            Ynodes  = c(&amp;quot;VL.0&amp;quot;,&amp;quot;VL.1&amp;quot;,&amp;quot;VL.2&amp;quot;,&amp;quot;VL.3&amp;quot;,&amp;quot;VL.4&amp;quot;),
            Anodes  = c(&amp;quot;efv.0&amp;quot;,&amp;quot;efv.1&amp;quot;,&amp;quot;efv.2&amp;quot;,&amp;quot;efv.3&amp;quot;,&amp;quot;efv.4&amp;quot;),
            Yweights = w$`0.01`,
            SL.library = list(c(&amp;quot;SL.glm&amp;quot;,&amp;quot;screen.glmnet_nVar_1_4_10_150&amp;quot;),
                              c(&amp;quot;SL.mgcv_base&amp;quot;, &amp;quot;screen.cramersv_3&amp;quot;)),
            SL.export = c(&amp;quot;SL.mgcv_base&amp;quot;,&amp;quot;screen.glmnet_nVar_base&amp;quot;,
                          &amp;quot;screen.cramersv_base&amp;quot;,&amp;quot;predict.SL.mgcv&amp;quot;),
            abar=seq(0,10,1), B=200, ncores=7, seed=41188, prog=&amp;quot;C:/temp&amp;quot;,
            cvControl = list(V=2), # SL option
            calc.support=TRUE
)
est5                                    # estimates
est5$SL.weights                         # Super Learner summary
plot(est5, time.points = c(1,5), CI=T)  # plot with CIs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;further-options&#34;&gt;Further options&lt;/h4&gt;
&lt;p&gt;The weighted sequential $g$-formula approach can also handle survival settings. There are many important subtleties related to the estimation of the conditional densities needed to calculate the outcome weights. It is possible
to customize density estimation with the highly-adaptive LASSO, and pass on many useful options; details will be described in the future manual.&lt;/p&gt;
&lt;h3 id=&#34;comparison&#34;&gt;Comparison&lt;/h3&gt;
&lt;p&gt;As a comparsion between the standard and weighted approach have a look at the below figure: in areas of low support (close to 0), the weighted curve is different, but otherwise similar. Details about the interpretation of the weighted curve
can be found at the bottom of Section 3.2.2 in the paper.&lt;/p&gt;
&lt;div&gt;&lt;img src=&#34;https://MichaelSchomaker.github.io/img/CICI_white.jpeg&#34;, width=100%&gt;&lt;/div&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;&lt;a name=&#34;code&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The code of this little tutorial is available &lt;a href=&#34;https://MichaelSchomaker.github.io/R/example_homepage.R&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;bottom&#34;&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SAE</title>
      <link>https://MichaelSchomaker.github.io/project/sae/</link>
      <pubDate>Thu, 27 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://MichaelSchomaker.github.io/project/sae/</guid>
      <description>&lt;p&gt;&lt;strong&gt;SAE - Shrinkage Averaging Estimation: some useful functions and an 
  &lt;i class=&#34;fab fa-r-project  pr-1 fa-fw&#34;&gt;&lt;/i&gt; package&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#Background&#34;&gt;$\rightarrow$ Background&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#Software&#34;&gt;$\rightarrow$ Software&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#other&#34;&gt;$\rightarrow$ Further considerations and literature&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;Background&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;
&lt;p&gt;It is now well-known that estimators post-model selection are typically both biased and overoptimistic in the sense that the uncertainty associated with the model selection process is typically not reflected
in confidence intervals. This is because the selected model is &lt;em&gt;conditional&lt;/em&gt; on the selection process; a different dataset, or a different selection method, may yield different conclusions.&lt;/p&gt;
&lt;p&gt;For many shrinkage estimators, the choice of one or many tuning parameters is required. Again, inference is conditional on the chosen tuning parameter. With model averaging, different &amp;ldquo;good&amp;rdquo; models are combined and the 
idea of shrinkage averaging estimation is to combine estimates resulting from different tuning parameter choices. Based on ideas from the optimal model averaging literature, one can think of clever combinations such that a cross-validation error is minimized. 
My 2012 paper explains the basic idea (see 
&lt;a href=&#34;https://MichaelSchomaker.github.io/publication/2012-01-01_shrinkage_averaging_/&#34;&gt;here&lt;/a&gt;), but our 2019 paper contains (see 
&lt;a href=&#34;https://MichaelSchomaker.github.io/publication/2019-01-01_when_and_when_not_to/&#34;&gt;here&lt;/a&gt;)
some more mature thoughts. These ideas are implemented in both my &lt;em&gt;SAE&lt;/em&gt; package as well as the &lt;em&gt;lae()&lt;/em&gt; function contained in my &lt;em&gt;MAMI&lt;/em&gt; package (see 
&lt;a href=&#34;https://MichaelSchomaker.github.io/project/mami/&#34;&gt;here&lt;/a&gt;). More details are given below.&lt;/p&gt;
&lt;h4 id=&#34;lasso-averaging-estimation&#34;&gt;LASSO averaging estimation&lt;/h4&gt;
&lt;p&gt;Consider the LASSO estimator for a simple linear model:
\begin{eqnarray}
\hat{\beta}_{\text{LE}}({\lambda}) &amp;amp;=&amp;amp; \text{arg} \min {\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p x_{ij}{{\beta}}_j)^2 + {\lambda} \sum_{j=1}^p |\beta_j| },.
\end{eqnarray}
The complexity parameter $\lambda \geq 0$ tunes the amount of shrinkage and is typically estimated via the generalized cross validation criterion (GCV) or any other cross validation criterion (CV). The larger the value of $\lambda$, the greater the amount of shrinkage since the estimated coefficients are shrunk towards zero.&lt;/p&gt;
&lt;p&gt;Consider a sequence of candidate tuning parameters $\boldsymbol{\lambda}={\lambda_1,\ldots,\lambda_L}$. 
If each estimator $\hat{\beta}_{\text{LE}}({\lambda_i})$ obtains a specific weight $w_{\lambda_i}$, then a &lt;em&gt;LASSO averaging estimator&lt;/em&gt; takes the form
\begin{eqnarray}
\hat{\bar{\beta}}_{\text{LAE}}&amp;amp;=&amp;amp; \sum_{i=1}^L w_{\lambda_i} \hat{\beta}_{\text{LE}}({\lambda_i}) = \mathbf{w}_\lambda \hat{\boldsymbol{B}}_{\text{LE}}  ,
\end{eqnarray}
where $\lambda_i \in [0,c]$, $c&amp;gt;0$ is a suitable constant, $\hat{{\boldsymbol{B}}}_{\text{LE}}=(\hat{\beta}_{\text{LE}}(\lambda_1),\ldots,\hat{\beta}_{\text{LE}}(\lambda_L))&#39;$ is the 
$L \times p$ matrix of the LASSO estimators, $\mathbf{w}_\lambda=(w_{\lambda_1},\ldots,w_{\lambda_L})$ is an $1 \times L$ weight vector, $\mathbf{w}_\lambda \in \mathcal{W}$ and $\mathcal{W}={\mathbf{w}_\lambda \in [0,1]^L: \mathbf{1}&#39;\mathbf{w}_\lambda=1}$.&lt;/p&gt;
&lt;p&gt;One could choose the weights
\begin{eqnarray}
\hat{\mathbf{w}}_\lambda^{\text{OCV}} &amp;amp;=&amp;amp; \text{arg} \min_{_{_{\mathbf{w}_\lambda\in\mathcal{W}}}} OCV_k
\end{eqnarray}
with
\begin{eqnarray}
OCV_k &amp;amp;=&amp;amp; \frac{1}{n}{\tilde{\boldsymbol\epsilon}_\kappa(w)}&#39; \tilde{\boldsymbol\epsilon}_\kappa(w) \nonumber\ &amp;amp;\propto&amp;amp; \mathbf{w}_{\lambda} \mathbf{E}&#39;_k \mathbf{E}_k {\mathbf{w}_{\lambda}}&#39; ,,
\end{eqnarray}
referring to an optimal cross validation (OCV) based criterion and $\mathbf{E}_k = (\tilde{\boldsymbol\epsilon}_k(\lambda_1),\ldots,\tilde{\boldsymbol\epsilon}_k(\lambda_L))$ is the $n \times L$ matrix
of the ($k$-fold) cross-validation residuals for the $L$ competing tuning parameters (given a specific loss function). An optimal weight vector for this criterion is then
\begin{eqnarray}
w^{\text{LAE}} &amp;amp;=&amp;amp; \text{arg} \min_{_{_{w\in\mathcal{W}}}} OCV_k .
\end{eqnarray}
These weights can be calculated with quadratic programming.&lt;/p&gt;
&lt;p&gt;Alternatively, the weight choice may not be based on predictive purposes, but on confidence interval coverage and resampling may be an option in this case.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;Software&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;software&#34;&gt;Software&lt;/h3&gt;
&lt;p&gt;Two implementations are available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the original files from 2012, summarized in the &lt;ins&gt; &lt;em&gt;SAE&lt;/em&gt; - package &lt;/ins&gt;, available  &lt;a href=&#34;https://MichaelSchomaker.github.io/zip/SAE_package.zip&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;a broader and more efficient implementation in the function &lt;em&gt;lae()&lt;/em&gt; in the 
&lt;a href=&#34;https://MichaelSchomaker.github.io/project/mami/&#34;&gt;&lt;em&gt;MAMI&lt;/em&gt; package&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Briefly, the new implementation is better and faster and can be used for the linear model, the logistic model and the Poisson model &amp;ndash; but not for longitudinal or survival data. It can be used for Ridge, Elastic Net and Lasso. 
The original SAE package can only be used for the linear model, but has both the bootstrap weight choice and Random Lasso option implemented (but not the Elastic Net). The use of the new files are encouraged, but the old files may be useful for reproduction
of results. There are many more options and possibilities, see Section 3.1.3 in the &lt;a href=&#34;https://MichaelSchomaker.github.io/pdf/MAMI_manual.pdf&#34; target=&#34;_blank&#34;&gt; MAMI manual&lt;/a&gt;, or under &lt;em&gt;?sae&lt;/em&gt; in the SAE package.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;other&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;further-considerations&#34;&gt;Further considerations&lt;/h3&gt;
&lt;p&gt;LASSO averaging is computationally very efficient. It is thus a fast prediction algorithm and suitable for inclusion into &lt;em&gt;super learning&lt;/em&gt;. 
Super learning means considering a set of prediction algorithms, for example regression models, 
shrinkage estimators or regression trees. Instead of choosing the algorithm with the smallest cross validation error, super learning chooses a weighted combination of different algorithms, 
that is the weighted combination which minimizes the cross validation error. It can be shown that this weighted combination will perform at least as good as the best algorithm, if not better.&lt;/p&gt;
&lt;p&gt;Briefly, &lt;em&gt;MAMI&lt;/em&gt; contains several shrinkage averaging estimation wrappers that can be used for super learning. They are listed and explained when typing&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;listSLWrappers()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is useful for both pure prediction problems and causal inference with targeted maximum likelihood estimation. The below 2019 reference (&amp;ldquo;When and when not to us eoptimal model avergaing&amp;rdquo;) contains more details.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Publications</title>
      <link>https://MichaelSchomaker.github.io/project/_old/publications/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://MichaelSchomaker.github.io/project/_old/publications/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Selected Projects</title>
      <link>https://MichaelSchomaker.github.io/project/_old/selected/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://MichaelSchomaker.github.io/project/_old/selected/</guid>
      <description>&lt;p&gt;$$x^2$$&lt;/p&gt;
&lt;p&gt;Test and &lt;em&gt;test&lt;/em&gt; and &lt;strong&gt;test&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Software</title>
      <link>https://MichaelSchomaker.github.io/project/_old/software/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://MichaelSchomaker.github.io/project/_old/software/</guid>
      <description>&lt;p&gt;$$x^2$$&lt;/p&gt;
&lt;p&gt;Test and &lt;em&gt;test&lt;/em&gt; and &lt;strong&gt;test&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>https://MichaelSchomaker.github.io/project/_old/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://MichaelSchomaker.github.io/project/_old/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
